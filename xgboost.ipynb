{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: numpy in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owen\\documents\\vscode\\xgboost-py\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn xgboost  -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "\n",
    "* 文字：https://www.zhihu.com/question/58883125/answer/2551395292\n",
    "* 视频：https://www.bilibili.com/video/BV1Zk4y1F7JE/\n",
    "* 代码：https://randomrealizations.com/posts/xgboost-from-scratch/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代表一颗树中的某个node\n",
    "class TreeBooster():\n",
    "    def __init__(self, X, g, h, params, max_depth, idxs=None):\n",
    "        # 各种超参数\n",
    "        self.params = params\n",
    "        self.max_depth = max_depth\n",
    "        assert self.max_depth >= 0, 'max_depth must be nonnegative'\n",
    "        self.min_child_weight = params['min_child_weight'] if params['min_child_weight'] else 1.0  \n",
    "        self.colsample_bynode = params['colsample_bynode'] if params['colsample_bynode'] else 1.0\n",
    "        \n",
    "        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0 # λ超参，公式的重要部分\n",
    "        self.gamma = params['gamma'] if params['gamma'] else 0.0 # γ超参，公式的重要部分\n",
    "        \n",
    "        # 每个样本的Loss一阶导数，常数\n",
    "        if isinstance(g, pd.Series):\n",
    "            g = g.values\n",
    "        # 每个样本的Loss二阶导数，常数\n",
    "        if isinstance(h, pd.Series): \n",
    "            h = h.values\n",
    "            \n",
    "        # 参与训练的样本index，默认所有\n",
    "        if idxs is None: \n",
    "            idxs = np.arange(len(g))\n",
    "            \n",
    "        # X：特征   g：Loss一阶导数   h：Loss二阶导数，idxs：参与训练的样本index\n",
    "        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n",
    "        \n",
    "        # n：样本数, c：特征数\n",
    "        self.n, self.c = len(idxs), X.shape[1] \n",
    "        \n",
    "        # Tree目标最小化的时候，Wj的取值计算.\n",
    "        # 如果当前Node就是叶子，那么其Wj就是所有传入的样本计算而成的\n",
    "        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n",
    "        self.best_score_so_far = 0.\n",
    "        \n",
    "        # 如果允许分裂节点，那么就继续递归\n",
    "        if self.max_depth > 0:\n",
    "            self._maybe_insert_child_nodes() # 为当前节点找到一个分裂条件，产生左右两颗子树，让样本落入两颗子树，每颗子树又可以继续更新自己的Wj或者继续递归分裂\n",
    "\n",
    "    def _maybe_insert_child_nodes(self):\n",
    "        # 遍历每一个特征，判断是否适合作为分裂条件\n",
    "        for i in range(self.c): \n",
    "            self._find_better_split(i)\n",
    "        \n",
    "        # 没找到合适的分裂点，那么就不分裂了，自己就是叶子，直接用value作为W值\n",
    "        if self.is_leaf: \n",
    "            return\n",
    "\n",
    "        # 作为分裂条件的特征\n",
    "        x = self.X.values[self.idxs,self.split_feature_idx]\n",
    "        \n",
    "        # <=分裂点的落入左子树\n",
    "        left_idx = np.nonzero(x <= self.threshold)[0]\n",
    "        # 其他落入右子树\n",
    "        right_idx = np.nonzero(x > self.threshold)[0]\n",
    "        # 递归构建左右子树\n",
    "        self.left = TreeBooster(self.X, self.g, self.h, self.params, self.max_depth - 1, self.idxs[left_idx])\n",
    "        self.right = TreeBooster(self.X, self.g, self.h, self.params, self.max_depth - 1, self.idxs[right_idx])\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): \n",
    "        return self.best_score_so_far == 0.\n",
    "    \n",
    "    def _find_better_split(self, feature_idx):\n",
    "        # 取出这一列特征\n",
    "        x = self.X.values[self.idxs, feature_idx]\n",
    "        # 取出所有样本的g和h导数\n",
    "        g, h = self.g[self.idxs], self.h[self.idxs]\n",
    "        \n",
    "        # 这一列特征值从小到大，对样本进行排序\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]  # g和h也跟随排序\n",
    "        \n",
    "        sum_g, sum_h = g.sum(), h.sum()\n",
    "        sum_g_right, sum_h_right = sum_g, sum_h\n",
    "        sum_g_left, sum_h_left = 0., 0.\n",
    "\n",
    "        # 遍历每一个样本\n",
    "        for i in range(0, self.n - 1):\n",
    "            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n",
    "            \n",
    "            # 以样本i的特征值作为分割点，计算H_left,G_left,H_right,G_right\n",
    "            sum_g_left += g_i; sum_g_right -= g_i\n",
    "            sum_h_left += h_i; sum_h_right -= h_i\n",
    "            \n",
    "            # 左子树w太小，则分裂点还得往右走\n",
    "            # 和下一个相邻样本取值一样，还得往右走\n",
    "            if sum_h_left < self.min_child_weight or x_i == x_i_next:\n",
    "                continue\n",
    "            # 分裂点继续往右，会导致右子树w太小，所以停止继续向右走\n",
    "            if sum_h_right < self.min_child_weight:\n",
    "                break\n",
    "            \n",
    "            # 求分裂和不分裂的目标值之差，寻找差距最大的，也就是分裂后目标值变的更小的\n",
    "            # TODO: 这里sum_g^2应该有问题，应该是g^2再求sum\n",
    "            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda)) + (sum_g_right**2 / (sum_h_right + self.reg_lambda)) - (sum_g**2 / (sum_h + self.reg_lambda))) - self.gamma/2 # Eq(7) in the xgboost paper\n",
    "            if gain > self.best_score_so_far: \n",
    "                self.split_feature_idx = feature_idx # 用哪个特征作为条件\n",
    "                self.best_score_so_far = gain\n",
    "                self.threshold = (x_i + x_i_next) / 2 # 用哪个值作为分割点\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n",
    "\n",
    "    # 预测就是根据树的结构，将样本落入到某个叶子节点，得到该叶子节点的Wj\n",
    "    def _predict_row(self, row):\n",
    "        if self.is_leaf: \n",
    "            return self.value\n",
    "        child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n",
    "        return child._predict_row(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 9, 4, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=np.random.default_rng(seed=None)\n",
    "r.choice(10,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel():\n",
    "    '''XGBoost from Scratch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, params, random_seed=None):\n",
    "        self.params = defaultdict(lambda: None, params)\n",
    "        self.subsample = self.params['subsample'] if self.params['subsample'] else 1.0\n",
    "        self.learning_rate = self.params['learning_rate'] if self.params['learning_rate'] else 0.3\n",
    "        self.base_prediction = self.params['base_score'] if self.params['base_score'] else 0.5\n",
    "        self.max_depth = self.params['max_depth'] if self.params['max_depth'] else 5\n",
    "        self.rng = np.random.default_rng(seed=random_seed)\n",
    "                \n",
    "    def fit(self, X, y, objective, num_boost_round, verbose=False):\n",
    "        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n",
    "        self.boosters = []\n",
    "        for i in range(num_boost_round):\n",
    "            gradients = objective.gradient(y, current_predictions) # 每个样本的Loss一阶导数，输入是目标值和累计预测值\n",
    "            hessians = objective.hessian(y, current_predictions)   # 每个样本的Loss二阶导数，输入是目标值和累计预测值\n",
    "            sample_idxs = None if self.subsample == 1.0 else self.rng.choice(len(y), size=math.floor(self.subsample*len(y)), replace=False) # 样本采样，这里先当全部样本都进入当前Tree的训练\n",
    "            booster = TreeBooster(X, gradients, hessians, self.params, self.max_depth, sample_idxs) # 根据导数和样本，以目标函数最优为目标，就能生成一棵树，然后以目标函数最优直接可以算出各叶子节点的W值\n",
    "            current_predictions += self.learning_rate * booster.predict(X)  # 更新预测值（不断boost提升而来）\n",
    "            self.boosters.append(booster)\n",
    "            if verbose: \n",
    "                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # 逐步提升预测值\n",
    "        return (self.base_prediction + self.learning_rate * np.sum([booster.predict(X) for booster in self.boosters], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train&Fit Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing,load_diabetes\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredErrorObjective():\n",
    "    # Loss\n",
    "    # (y_i-pred_i_minus_1)^2 / 2\n",
    "    def loss(self, y, pred):\n",
    "        return np.mean(((y - pred)**2)*0.5)\n",
    "    \n",
    "    # Loss关于Pred的一阶导数\n",
    "    def gradient(self, y, pred):\n",
    "        return pred - y\n",
    "    \n",
    "    # Loss关于Pred的二阶导数\n",
    "    def hessian(self, y, pred):\n",
    "        return np.ones(len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_12584\\3180208859.py:109: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train loss = 12067.915096331782\n",
      "[1] train loss = 10136.583004712125\n",
      "[2] train loss = 8545.366131953378\n",
      "[3] train loss = 7271.650873197832\n",
      "[4] train loss = 6196.441498770383\n",
      "[5] train loss = 5309.535034301482\n",
      "[6] train loss = 4593.178107892641\n",
      "[7] train loss = 3981.2698016685486\n",
      "[8] train loss = 3485.232368552386\n",
      "[9] train loss = 3080.517639532801\n",
      "[10] train loss = 2763.317235977239\n",
      "[11] train loss = 2479.901281262155\n",
      "[12] train loss = 2247.7882975853536\n",
      "[13] train loss = 2036.3670195745376\n",
      "[14] train loss = 1868.6455726836625\n",
      "[15] train loss = 1718.6056806848126\n",
      "[16] train loss = 1613.5275610830367\n",
      "[17] train loss = 1521.2439404167922\n",
      "[18] train loss = 1444.2667372016956\n",
      "[19] train loss = 1385.2613768423853\n",
      "[20] train loss = 1333.6334505092027\n",
      "[21] train loss = 1289.3456542502825\n",
      "[22] train loss = 1248.1174709576865\n",
      "[23] train loss = 1211.5847198938145\n",
      "[24] train loss = 1176.8495343540578\n",
      "[25] train loss = 1142.691063050909\n",
      "[26] train loss = 1109.600296052089\n",
      "[27] train loss = 1081.0749398918813\n",
      "[28] train loss = 1061.1882234795341\n",
      "[29] train loss = 1043.0710842886667\n",
      "[30] train loss = 1024.7672048391546\n",
      "[31] train loss = 1009.4160690141782\n",
      "[32] train loss = 993.8415328653896\n",
      "[33] train loss = 977.3227809552425\n",
      "[34] train loss = 962.6009070686778\n",
      "[35] train loss = 952.8493922331317\n",
      "[36] train loss = 939.8110495302284\n",
      "[37] train loss = 928.2758966629257\n",
      "[38] train loss = 914.997341734202\n",
      "[39] train loss = 905.8660976652121\n",
      "[40] train loss = 898.528775571642\n",
      "[41] train loss = 888.2987439878643\n",
      "[42] train loss = 873.3959964324491\n",
      "[43] train loss = 865.0452430903231\n",
      "[44] train loss = 857.2701310676918\n",
      "[45] train loss = 850.1538017505775\n",
      "[46] train loss = 840.7379936596627\n",
      "[47] train loss = 835.1193017512076\n",
      "[48] train loss = 822.6053653429531\n",
      "[49] train loss = 813.4805934876526\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'reg_lambda': 1.5,\n",
    "    'gamma': 0.0,\n",
    "    'min_child_weight': 25,\n",
    "    'base_score': 0.0,\n",
    "    'tree_method': 'exact',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "# train the from-scratch XGBoost model\n",
    "model_scratch = XGBoostModel(params, random_seed=42)\n",
    "model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round, verbose=True)\n",
    "\n",
    "# train the library XGBoost model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "model_xgb = xgb.train(params, dtrain, num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_12584\\3180208859.py:109: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch score: 1732.3661562522236\n",
      "xgboost score: 1679.5604874796754\n"
     ]
    }
   ],
   "source": [
    "pred_scratch = model_scratch.predict(X_test)\n",
    "pred_xgb = model_xgb.predict(dtest)\n",
    "print(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\n",
    "print(f'xgboost score: {SquaredErrorObjective().loss(y_test, pred_xgb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train&Fit Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "X, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补充实现BCE二值交叉熵损失【二分类问题】\n",
    "class BinaryCrossEntropybjective():\n",
    "    # BCE损失函数与导数：https://blog.csdn.net/zzl12880/article/details/128403845\n",
    "    def loss(self, y, pred):\n",
    "        return -(y * np.log(1/(1+np.exp(-pred))) + (1 - y) * np.log(1 - np.exp(-pred)))\n",
    "    \n",
    "    # Loss关于Pred的一阶导数\n",
    "    def gradient(self, y, pred):\n",
    "        return 1/(1+np.exp(-pred)) - y\n",
    "    \n",
    "    # Loss关于Pred的二阶导数\n",
    "    def hessian(self, y, pred):\n",
    "        sigmoid_value= 1/(1+np.exp(-pred))\n",
    "        return sigmoid_value * (1 - sigmoid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] train loss = 0.1060660886856477\n",
      "[1] train loss = 0.08991028056243167\n",
      "[2] train loss = 0.07687713770725316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_12584\\3180208859.py:109: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] train loss = 0.06637490107938446\n",
      "[4] train loss = 0.05769830250626569\n",
      "[5] train loss = 0.05064251584502835\n",
      "[6] train loss = 0.04497041647222459\n",
      "[7] train loss = 0.03995087548592409\n",
      "[8] train loss = 0.035637172643063236\n",
      "[9] train loss = 0.031962604868957446\n",
      "[10] train loss = 0.02907202358019898\n",
      "[11] train loss = 0.026439477188828806\n",
      "[12] train loss = 0.024196875272845416\n",
      "[13] train loss = 0.022560767467964957\n",
      "[14] train loss = 0.021055052626616274\n",
      "[15] train loss = 0.019604384577369217\n",
      "[16] train loss = 0.018334770703077253\n",
      "[17] train loss = 0.017249783496337146\n",
      "[18] train loss = 0.016172061000162764\n",
      "[19] train loss = 0.01515978274254834\n",
      "[20] train loss = 0.014465078974169149\n",
      "[21] train loss = 0.013775881480289846\n",
      "[22] train loss = 0.013130769985621476\n",
      "[23] train loss = 0.012588915434374848\n",
      "[24] train loss = 0.012052456530451698\n",
      "[25] train loss = 0.011580432959313136\n",
      "[26] train loss = 0.011052727805393233\n",
      "[27] train loss = 0.010684269216835235\n",
      "[28] train loss = 0.010323123759768101\n",
      "[29] train loss = 0.009927526313820881\n",
      "[30] train loss = 0.009532793030233369\n",
      "[31] train loss = 0.009159710797235668\n",
      "[32] train loss = 0.008918884137684522\n",
      "[33] train loss = 0.008710912413736694\n",
      "[34] train loss = 0.00852117105826099\n",
      "[35] train loss = 0.008252145524603575\n",
      "[36] train loss = 0.008069650103027837\n",
      "[37] train loss = 0.007768869348665925\n",
      "[38] train loss = 0.007492306162330999\n",
      "[39] train loss = 0.007319822899915765\n",
      "[40] train loss = 0.007103194890782302\n",
      "[41] train loss = 0.006832962979158726\n",
      "[42] train loss = 0.006590510045509175\n",
      "[43] train loss = 0.006415386286017237\n",
      "[44] train loss = 0.006290993972724145\n",
      "[45] train loss = 0.00617684708264705\n",
      "[46] train loss = 0.006056375191996022\n",
      "[47] train loss = 0.005959572649301144\n",
      "[48] train loss = 0.005878743885260679\n",
      "[49] train loss = 0.0058087165146744595\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'reg_lambda': 1.5,\n",
    "    'gamma': 0.0,\n",
    "    'min_child_weight': 25,\n",
    "    'base_score': 0.0,\n",
    "    'tree_method': 'exact',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "# train the from-scratch XGBoost model\n",
    "model_scratch = XGBoostModel(params, random_seed=42)\n",
    "model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round, verbose=True)\n",
    "\n",
    "# train the library XGBoost model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "model_xgb = xgb.train(params, dtrain, num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owen\\AppData\\Local\\Temp\\ipykernel_12584\\3180208859.py:109: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch score: 0.01880803263442271\n",
      "xgboost score: 0.020792682100083962\n"
     ]
    }
   ],
   "source": [
    "pred_scratch = model_scratch.predict(X_test)\n",
    "pred_xgb = model_xgb.predict(dtest)\n",
    "print(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\n",
    "print(f'xgboost score: {SquaredErrorObjective().loss(y_test, pred_xgb)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch acc:86.54970760233918\n",
      "xgboost acc:87.71929824561403\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_test,pred):\n",
    "    # 概率>0.5的算作分类1，其他分类0\n",
    "    probs=1/(1+np.exp(-pred))\n",
    "    classes=np.zeros_like(probs)\n",
    "    classes[probs>0.5]=1\n",
    "\n",
    "    # 计算准确率\n",
    "    acc=(classes==y_test).sum()/len(classes)*100\n",
    "    # numpy转float \n",
    "    return acc.item()\n",
    "\n",
    "print(f'scratch acc:{accuracy(y_test,pred_scratch)}')\n",
    "print(f'xgboost acc:{accuracy(y_test,pred_xgb)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
